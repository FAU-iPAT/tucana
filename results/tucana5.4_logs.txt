
#################################
###   Start training script   ###
#################################

decay            = None
maxfilecount     = 1024
configpath       = None
verbose          = 2
datapath         = None
nocache          = 1
initialepoch     = 0
learningrate     = None
tensorboard      = None
fileformat       = batch_{0:05d}.npy
weightfile       = None
usepoisson       = 0
mindist          = 0.0
epochs           = 150
usebartlett      = 0
userectangle     = 1
runstatsfile     = None
histogramfreq    = 0
databasepath     = ./
bestfile         = None
cprunstatsfile   = None
usemeyer         = 0
checkpointfile   = None
resumefile       = None
modelfile        = ./tucana_v5.4.json
answerpath       = None
checkpoint       = 1
resultpath       = ./results5.4/data.windows/rh/
batchsize        = 128
usehanning       = 1

###############################
###   Importing libraries   ###
###############################


#########################################
###   Checking setup and parameters   ###
#########################################

Model File = ./tucana_v5.4.json
Data Path = ./rectangle/
Data Path = ./hanning/
Answer Path = ./answer/
Config Path = ./config/
File Format = batch_{0:05d}.npy

Best File = ./results5.4/data.windows/rh/best.hdf5
Final Weights File = ./results5.4/data.windows/rh/final.hdf5
Tensorboard Path = ./results5.4/data.windows/rh/
Runtime Statistic File = ./results5.4/data.windows/rh/runstats.npy

Batch Size = 128

Minimal Distance = 0.000

####################################
###   Load and compiling model   ###
####################################

__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input (InputLayer)              (None, 256, 6)       0                                            
__________________________________________________________________________________________________
recombine1 (Conv1D)             (None, 256, 29)      203         input[0][0]                      
__________________________________________________________________________________________________
conv1 (Conv1D)                  (None, 256, 29)      551         input[0][0]                      
__________________________________________________________________________________________________
data2 (Concatenate)             (None, 256, 64)      0           input[0][0]                      
                                                                 recombine1[0][0]                 
                                                                 conv1[0][0]                      
__________________________________________________________________________________________________
recombine2 (Conv1D)             (None, 256, 32)      2080        data2[0][0]                      
__________________________________________________________________________________________________
conv2 (Conv1D)                  (None, 256, 32)      6176        data2[0][0]                      
__________________________________________________________________________________________________
data3 (Concatenate)             (None, 256, 128)     0           data2[0][0]                      
                                                                 recombine2[0][0]                 
                                                                 conv2[0][0]                      
__________________________________________________________________________________________________
inception1_pool1 (MaxPooling1D) (None, 256, 128)     0           data3[0][0]                      
__________________________________________________________________________________________________
inception1_3x3_1 (Conv1D)       (None, 256, 16)      2064        data3[0][0]                      
__________________________________________________________________________________________________
inception1_5x5_1 (Conv1D)       (None, 256, 16)      2064        data3[0][0]                      
__________________________________________________________________________________________________
inception1_pool2 (Conv1D)       (None, 256, 32)      4128        inception1_pool1[0][0]           
__________________________________________________________________________________________________
inception1_1x1 (Conv1D)         (None, 256, 32)      4128        data3[0][0]                      
__________________________________________________________________________________________________
inception1_3x3_2 (Conv1D)       (None, 256, 32)      1568        inception1_3x3_1[0][0]           
__________________________________________________________________________________________________
inception1_5x5_2 (Conv1D)       (None, 256, 32)      2592        inception1_5x5_1[0][0]           
__________________________________________________________________________________________________
inception1_result (Concatenate) (None, 256, 128)     0           inception1_pool2[0][0]           
                                                                 inception1_1x1[0][0]             
                                                                 inception1_3x3_2[0][0]           
                                                                 inception1_5x5_2[0][0]           
__________________________________________________________________________________________________
inception2_pool1 (MaxPooling1D) (None, 256, 128)     0           inception1_result[0][0]          
__________________________________________________________________________________________________
inception2_3x3_1 (Conv1D)       (None, 256, 16)      2064        inception1_result[0][0]          
__________________________________________________________________________________________________
inception2_5x5_1 (Conv1D)       (None, 256, 16)      2064        inception1_result[0][0]          
__________________________________________________________________________________________________
inception2_pool2 (Conv1D)       (None, 256, 32)      4128        inception2_pool1[0][0]           
__________________________________________________________________________________________________
inception2_1x1 (Conv1D)         (None, 256, 32)      4128        inception1_result[0][0]          
__________________________________________________________________________________________________
inception2_3x3_2 (Conv1D)       (None, 256, 32)      1568        inception2_3x3_1[0][0]           
__________________________________________________________________________________________________
inception2_5x5_2 (Conv1D)       (None, 256, 32)      2592        inception2_5x5_1[0][0]           
__________________________________________________________________________________________________
inception2_result (Concatenate) (None, 256, 128)     0           inception2_pool2[0][0]           
                                                                 inception2_1x1[0][0]             
                                                                 inception2_3x3_2[0][0]           
                                                                 inception2_5x5_2[0][0]           
__________________________________________________________________________________________________
inception3_pool1 (MaxPooling1D) (None, 256, 128)     0           inception2_result[0][0]          
__________________________________________________________________________________________________
inception3_3x3_1 (Conv1D)       (None, 256, 16)      2064        inception2_result[0][0]          
__________________________________________________________________________________________________
inception3_5x5_1 (Conv1D)       (None, 256, 16)      2064        inception2_result[0][0]          
__________________________________________________________________________________________________
inception3_pool2 (Conv1D)       (None, 256, 32)      4128        inception3_pool1[0][0]           
__________________________________________________________________________________________________2016-05-06 05:57:30.410752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:857] ARM64 does not support NUMA - returning NUMA node zero
2016-05-06 05:57:30.414225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: NVIDIA Tegra X2
major: 6 minor: 2 memoryClockRate (GHz) 1.3005
pciBusID 0000:00:00.0
Total memory: 7.67GiB
Free memory: 6.90GiB
2016-05-06 05:57:30.414282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2016-05-06 05:57:30.414311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2016-05-06 05:57:30.414344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0)

inception3_1x1 (Conv1D)         (None, 256, 32)      4128        inception2_result[0][0]          
__________________________________________________________________________________________________
inception3_3x3_2 (Conv1D)       (None, 256, 32)      1568        inception3_3x3_1[0][0]           
__________________________________________________________________________________________________
inception3_5x5_2 (Conv1D)       (None, 256, 32)      2592        inception3_5x5_1[0][0]           
__________________________________________________________________________________________________
inception3_result (Concatenate) (None, 256, 128)     0           inception3_pool2[0][0]           
                                                                 inception3_1x1[0][0]             
                                                                 inception3_3x3_2[0][0]           
                                                                 inception3_5x5_2[0][0]           
__________________________________________________________________________________________________
reduce (Conv1D)                 (None, 256, 32)      4128        inception3_result[0][0]          
__________________________________________________________________________________________________
dropout1 (Dropout)              (None, 256, 32)      0           reduce[0][0]                     
__________________________________________________________________________________________________
final1 (Conv1D)                 (None, 256, 24)      2328        dropout1[0][0]                   
__________________________________________________________________________________________________
dropout2 (Dropout)              (None, 256, 24)      0           final1[0][0]                     
__________________________________________________________________________________________________
final2 (Conv1D)                 (None, 256, 16)      1168        dropout2[0][0]                   
__________________________________________________________________________________________________
final3 (Conv1D)                 (None, 256, 1)       17          final2[0][0]                     
__________________________________________________________________________________________________
output (Reshape)                (None, 256)          0           final3[0][0]                     
==================================================================================================
Total params: 66,283
Trainable params: 66,283
Non-trainable params: 0
__________________________________________________________________________________________________

###############################
###   Building generators   ###
###############################

Creating new DataGenerator class:
   !!! WARNING !!! Clearing linux cache/buffer is enabled!
File format          = batch_{0:05d}.npy
Data path            = ./rectangle/
Data path            = ./hanning/
Answer path          = ./answer/
Config path          = ./config/
Number of files      =     1024
Entries per file     =     1024
Default batch size   =      128
Batches per file     =        8

#################################
###   Counting data samples   ###
#################################

Valid Training Samples = 734208   (100.00%)
Valid Validation Samples = 209920   (100.00%)
Valid Testing Samples = 104448   (100.00%)

#################################
###   Run training of model   ###
#################################

Epoch 1/150
 - 1701s - loss: 0.0868 - binary_accuracy: 0.9762 - val_loss: 0.0789 - val_binary_accuracy: 0.9786
Epoch 2/150
 - 1629s - loss: 0.0784 - binary_accuracy: 0.9784 - val_loss: 0.0784 - val_binary_accuracy: 0.9788
Epoch 3/150
 - 1631s - loss: 0.0773 - binary_accuracy: 0.9787 - val_loss: 0.0788 - val_binary_accuracy: 0.9787
Epoch 4/150
 - 1636s - loss: 0.0768 - binary_accuracy: 0.9788 - val_loss: 0.0798 - val_binary_accuracy: 0.9785
Epoch 5/150
 - 1625s - loss: 0.0765 - binary_accuracy: 0.9789 - val_loss: 0.0795 - val_binary_accuracy: 0.9780
Epoch 6/150
 - 1639s - loss: 0.0762 - binary_accuracy: 0.9790 - val_loss: 0.0804 - val_binary_accuracy: 0.9779
Epoch 7/150
 - 1648s - loss: 0.0760 - binary_accuracy: 0.9790 - val_loss: 0.0799 - val_binary_accuracy: 0.9777
Epoch 8/150
 - 1637s - loss: 0.0758 - binary_accuracy: 0.9791 - val_loss: 0.0790 - val_binary_accuracy: 0.9777
Epoch 9/150
 - 1680s - loss: 0.0758 - binary_accuracy: 0.9791 - val_loss: 0.0799 - val_binary_accuracy: 0.9779
Epoch 10/150
 - 1627s - loss: 0.0757 - binary_accuracy: 0.9791 - val_loss: 0.0788 - val_binary_accuracy: 0.9778
Epoch 11/150
 - 1624s - loss: 0.0756 - binary_accuracy: 0.9792 - val_loss: 0.0783 - val_binary_accuracy: 0.9784
Epoch 12/150
 - 1640s - loss: 0.0755 - binary_accuracy: 0.9792 - val_loss: 0.0789 - val_binary_accuracy: 0.9780
Epoch 13/150
 - 1636s - loss: 0.0755 - binary_accuracy: 0.9792 - val_loss: 0.0789 - val_binary_accuracy: 0.9780
Epoch 14/150
 - 1641s - loss: 0.0754 - binary_accuracy: 0.9792 - val_loss: 0.0791 - val_binary_accuracy: 0.9781
Epoch 15/150
 - 1630s - loss: 0.0754 - binary_accuracy: 0.9792 - val_loss: 0.0801 - val_binary_accuracy: 0.9782
Epoch 16/150
 - 1632s - loss: 0.0753 - binary_accuracy: 0.9792 - val_loss: 0.0796 - val_binary_accuracy: 0.9781
Epoch 17/150
 - 1653s - loss: 0.0753 - binary_accuracy: 0.9793 - val_loss: 0.0785 - val_binary_accuracy: 0.9785
Epoch 18/150
 - 1633s - loss: 0.0752 - binary_accuracy: 0.9793 - val_loss: 0.0791 - val_binary_accuracy: 0.9782
Epoch 19/150
 - 1637s - loss: 0.0752 - binary_accuracy: 0.9793 - val_loss: 0.0789 - val_binary_accuracy: 0.9787
Epoch 20/150
 - 1639s - loss: 0.0752 - binary_accuracy: 0.9793 - val_loss: 0.0791 - val_binary_accuracy: 0.9781
Epoch 21/150
 - 1640s - loss: 0.0752 - binary_accuracy: 0.9793 - val_loss: 0.0788 - val_binary_accuracy: 0.9781
Epoch 22/150
 - 1625s - loss: 0.0751 - binary_accuracy: 0.9793 - val_loss: 0.0782 - val_binary_accuracy: 0.9784
Epoch 23/150
 - 1637s - loss: 0.0751 - binary_accuracy: 0.9793 - val_loss: 0.0793 - val_binary_accuracy: 0.9784
Epoch 24/150
 - 1627s - loss: 0.0751 - binary_accuracy: 0.9793 - val_loss: 0.0783 - val_binary_accuracy: 0.9786
Epoch 25/150
 - 1628s - loss: 0.0750 - binary_accuracy: 0.9793 - val_loss: 0.0784 - val_binary_accuracy: 0.9785
Epoch 26/150
 - 1625s - loss: 0.0750 - binary_accuracy: 0.9793 - val_loss: 0.0784 - val_binary_accuracy: 0.9786
Epoch 27/150
 - 1633s - loss: 0.0750 - binary_accuracy: 0.9793 - val_loss: 0.0779 - val_binary_accuracy: 0.9787
Epoch 28/150
 - 1646s - loss: 0.0750 - binary_accuracy: 0.9793 - val_loss: 0.0776 - val_binary_accuracy: 0.9788
Epoch 29/150
 - 1637s - loss: 0.0749 - binary_accuracy: 0.9794 - val_loss: 0.0794 - val_binary_accuracy: 0.9783
Epoch 30/150
 - 1633s - loss: 0.0749 - binary_accuracy: 0.9794 - val_loss: 0.0784 - val_binary_accuracy: 0.9788
Epoch 31/150
 - 1628s - loss: 0.0749 - binary_accuracy: 0.9794 - val_loss: 0.0788 - val_binary_accuracy: 0.9783
Epoch 32/150
 - 1631s - loss: 0.0749 - binary_accuracy: 0.9794 - val_loss: 0.0777 - val_binary_accuracy: 0.9788
Epoch 33/150
 - 1663s - loss: 0.0749 - binary_accuracy: 0.9794 - val_loss: 0.0779 - val_binary_accuracy: 0.9787
Epoch 34/150
 - 1627s - loss: 0.0749 - binary_accuracy: 0.9794 - val_loss: 0.0807 - val_binary_accuracy: 0.9781
Epoch 35/150
 - 1658s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0792 - val_binary_accuracy: 0.9783
Epoch 36/150
 - 1629s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0797 - val_binary_accuracy: 0.9783
Epoch 37/150
 - 1642s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0796 - val_binary_accuracy: 0.9781
Epoch 38/150
 - 1630s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0790 - val_binary_accuracy: 0.9785
Epoch 39/150
 - 1630s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0785 - val_binary_accuracy: 0.9785
Epoch 40/150
 - 1624s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0791 - val_binary_accuracy: 0.9786
Epoch 41/150
 - 1627s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0787 - val_binary_accuracy: 0.9787
Epoch 42/150
 - 1626s - loss: 0.0748 - binary_accuracy: 0.9794 - val_loss: 0.0781 - val_binary_accuracy: 0.9786
Epoch 43/150
 - 1625s - loss: 0.0747 - binary_accuracy: 0.9794 - val_loss: 0.0794 - val_binary_accuracy: 0.9782
Epoch 44/150
 - 1643s - loss: 0.0747 - binary_accuracy: 0.9794 - val_loss: 0.0788 - val_binary_accuracy: 0.9788
Epoch 45/150
 - 1630s - loss: 0.0747 - binary_accuracy: 0.9794 - val_loss: 0.0785 - val_binary_accuracy: 0.9784
Epoch 46/150
 - 1635s - loss: 0.0747 - binary_accuracy: 0.9794 - val_loss: 0.0790 - val_binary_accuracy: 0.9786
Epoch 47/150
 - 1648s - loss: 0.0747 - binary_accuracy: 0.9794 - val_loss: 0.0794 - val_binary_accuracy: 0.9779
Epoch 48/150
 - 1638s - loss: 0.0747 - binary_accuracy: 0.9794 - val_loss: 0.0790 - val_binary_accuracy: 0.9785
Epoch 49/150
 - 1635s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0788 - val_binary_accuracy: 0.9787
Epoch 50/150
 - 1627s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0795 - val_binary_accuracy: 0.9777
Epoch 51/150
 - 1646s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0771 - val_binary_accuracy: 0.9789
Epoch 52/150
 - 1669s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0792 - val_binary_accuracy: 0.9782
Epoch 53/150
 - 1627s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0788 - val_binary_accuracy: 0.9786
Epoch 54/150
 - 1651s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0804 - val_binary_accuracy: 0.9779
Epoch 55/150
 - 1629s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0793 - val_binary_accuracy: 0.9780
Epoch 56/150
 - 1624s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0802 - val_binary_accuracy: 0.9772
Epoch 57/150
 - 1634s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0794 - val_binary_accuracy: 0.9784
Epoch 58/150
 - 1646s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0782 - val_binary_accuracy: 0.9783
Epoch 59/150
 - 1625s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0789 - val_binary_accuracy: 0.9782
Epoch 60/150
 - 1629s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0796 - val_binary_accuracy: 0.9777
Epoch 61/150
 - 1632s - loss: 0.0746 - binary_accuracy: 0.9795 - val_loss: 0.0785 - val_binary_accuracy: 0.9779
Epoch 62/150
 - 1622s - loss: 0.0746 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9781
Epoch 63/150
 - 1624s - loss: 0.0746 - binary_accuracy: 0.9794 - val_loss: 0.0785 - val_binary_accuracy: 0.9782
Epoch 64/150
 - 1628s - loss: 0.0746 - binary_accuracy: 0.9795 - val_loss: 0.0800 - val_binary_accuracy: 0.9782
Epoch 65/150
 - 1630s - loss: 0.0746 - binary_accuracy: 0.9795 - val_loss: 0.0795 - val_binary_accuracy: 0.9778
Epoch 66/150
 - 1637s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0790 - val_binary_accuracy: 0.9785
Epoch 67/150
 - 1634s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0785 - val_binary_accuracy: 0.9785
Epoch 68/150
 - 1619s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9778
Epoch 69/150
 - 1629s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9783
Epoch 70/150
 - 1647s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0796 - val_binary_accuracy: 0.9781
Epoch 71/150
 - 1632s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0787 - val_binary_accuracy: 0.9783
Epoch 72/150
 - 1636s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0793 - val_binary_accuracy: 0.9783
Epoch 73/150
 - 1632s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0788 - val_binary_accuracy: 0.9784
Epoch 74/150
 - 1634s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0789 - val_binary_accuracy: 0.9783
Epoch 75/150
 - 1625s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0799 - val_binary_accuracy: 0.9779
Epoch 76/150
 - 1649s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0787 - val_binary_accuracy: 0.9785
Epoch 77/150
 - 1643s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0793 - val_binary_accuracy: 0.9781
Epoch 78/150
 - 1619s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9781
Epoch 79/150
 - 1620s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0800 - val_binary_accuracy: 0.9782
Epoch 80/150
 - 1626s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0796 - val_binary_accuracy: 0.9784
Epoch 81/150
 - 1623s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0796 - val_binary_accuracy: 0.9780
Epoch 82/150
 - 1631s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0793 - val_binary_accuracy: 0.9783
Epoch 83/150
 - 1618s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0800 - val_binary_accuracy: 0.9782
Epoch 84/150
 - 1621s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9780
Epoch 85/150
 - 1642s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0795 - val_binary_accuracy: 0.9782
Epoch 86/150
 - 1630s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0796 - val_binary_accuracy: 0.9780
Epoch 87/150
 - 1628s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0791 - val_binary_accuracy: 0.9785
Epoch 88/150
 - 1624s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0794 - val_binary_accuracy: 0.9777
Epoch 89/150
 - 1628s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0798 - val_binary_accuracy: 0.9778
Epoch 90/150
 - 1637s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0796 - val_binary_accuracy: 0.9779
Epoch 91/150
 - 1631s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0789 - val_binary_accuracy: 0.9781
Epoch 92/150
 - 1624s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0787 - val_binary_accuracy: 0.9780
Epoch 93/150
 - 1623s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0801 - val_binary_accuracy: 0.9780
Epoch 94/150
 - 1622s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0789 - val_binary_accuracy: 0.9784
Epoch 95/150
 - 1631s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0785 - val_binary_accuracy: 0.9784
Epoch 96/150
 - 1624s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0791 - val_binary_accuracy: 0.9781
Epoch 97/150
 - 1624s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0780 - val_binary_accuracy: 0.9782
Epoch 98/150
 - 1619s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0799 - val_binary_accuracy: 0.9781
Epoch 99/150
 - 1623s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9777
Epoch 100/150
 - 1616s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0796 - val_binary_accuracy: 0.9777
Epoch 101/150
 - 1619s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0782 - val_binary_accuracy: 0.9784
Epoch 102/150
 - 1614s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9779
Epoch 103/150
 - 1619s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0781 - val_binary_accuracy: 0.9783
Epoch 104/150
 - 1625s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0787 - val_binary_accuracy: 0.9781
Epoch 105/150
 - 1615s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9777
Epoch 106/150
 - 1633s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0799 - val_binary_accuracy: 0.9777
Epoch 107/150
 - 1636s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0800 - val_binary_accuracy: 0.9781
Epoch 108/150
 - 1616s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0786 - val_binary_accuracy: 0.9780
Epoch 109/150
 - 1614s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0783 - val_binary_accuracy: 0.9783
Epoch 110/150
 - 1645s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9779
Epoch 111/150
 - 1633s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0808 - val_binary_accuracy: 0.9778
Epoch 112/150
 - 1640s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0788 - val_binary_accuracy: 0.9779
Epoch 113/150
 - 1625s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0802 - val_binary_accuracy: 0.9778
Epoch 114/150
 - 1608s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0784 - val_binary_accuracy: 0.9783
Epoch 115/150
 - 1614s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9783
Epoch 116/150
 - 1610s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9782
Epoch 117/150
 - 1609s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0789 - val_binary_accuracy: 0.9780
Epoch 118/150
 - 1620s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9782
Epoch 119/150
 - 1607s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0799 - val_binary_accuracy: 0.9780
Epoch 120/150
 - 1616s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0781 - val_binary_accuracy: 0.9784
Epoch 121/150
 - 1611s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0800 - val_binary_accuracy: 0.9777
Epoch 122/150
 - 1604s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0782 - val_binary_accuracy: 0.9782
Epoch 123/150
 - 1622s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0791 - val_binary_accuracy: 0.9779
Epoch 124/150
 - 1606s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0805 - val_binary_accuracy: 0.9780
Epoch 125/150
 - 1608s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0790 - val_binary_accuracy: 0.9778
Epoch 126/150
 - 1619s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0788 - val_binary_accuracy: 0.9781
Epoch 127/150
 - 1616s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9782
Epoch 128/150
 - 1603s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0793 - val_binary_accuracy: 0.9785
Epoch 129/150
 - 1599s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9783
Epoch 130/150
 - 1609s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0808 - val_binary_accuracy: 0.9777
Epoch 131/150
 - 1624s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0787 - val_binary_accuracy: 0.9782
Epoch 132/150
 - 1626s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0801 - val_binary_accuracy: 0.9779
Epoch 133/150
 - 1608s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0806 - val_binary_accuracy: 0.9778
Epoch 134/150
 - 1599s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0789 - val_binary_accuracy: 0.9782
Epoch 135/150
 - 1608s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0782 - val_binary_accuracy: 0.9782
Epoch 136/150
 - 1633s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0791 - val_binary_accuracy: 0.9783
Epoch 137/150
 - 1623s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0794 - val_binary_accuracy: 0.9780
Epoch 138/150
 - 1609s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0785 - val_binary_accuracy: 0.9782
Epoch 139/150
 - 1606s - loss: 0.0743 - binary_accuracy: 0.9795 - val_loss: 0.0794 - val_binary_accuracy: 0.9780
Epoch 140/150
 - 1607s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0781 - val_binary_accuracy: 0.9782
Epoch 141/150
 - 1615s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0781 - val_binary_accuracy: 0.9784
Epoch 142/150
 - 1603s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0797 - val_binary_accuracy: 0.9785
Epoch 143/150
 - 1625s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0817 - val_binary_accuracy: 0.9777
Epoch 144/150
 - 1622s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0793 - val_binary_accuracy: 0.9781
Epoch 145/150
 - 1599s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0788 - val_binary_accuracy: 0.9781
Epoch 146/150
 - 1599s - loss: 0.0744 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9782
Epoch 147/150
 - 1607s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0793 - val_binary_accuracy: 0.9781
Epoch 148/150
 - 1599s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0792 - val_binary_accuracy: 0.9786
Epoch 149/150
 - 1599s - loss: 0.0745 - binary_accuracy: 0.9795 - val_loss: 0.0798 - val_binary_accuracy: 0.9782
Epoch 150/150
 - 1607s - loss: 0.0746 - binary_accuracy: 0.9795 - val_loss: 0.0789 - val_binary_accuracy: 0.9777
Using TensorFlow backend.
train_tucana_v5.4.py:418: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=150, initial_epoch=0, max_queue_size=15, validation_data=<generator..., validation_steps=1640, callbacks=[<dieFFT.t..., verbose=2, steps_per_epoch=5736)`
  initial_epoch=config_initial_epoch,

################################################
###   Assembling stats and saving runstats   ###
################################################

Statistics of best model (best validation accuracy):
Training accuracy         =  97.89%
Validation accuracy       =  97.89%
Testing accuracy          =  97.88%

###############################
###   End training script   ###
###############################

